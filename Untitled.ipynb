{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce80c86-76c4-4344-9a01-758fe50aee89",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m    100\u001b[39m anno_file_path = os.path.join(DATA_PATH, \u001b[33m\"\u001b[39m\u001b[33mlabelv2\u001b[39m\u001b[33m\"\u001b[39m, subset, \u001b[33m\"\u001b[39m\u001b[33mlabelv2.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m image_dir = os.path.join(DATA_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWIDER_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWIDER_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m container = \u001b[43mAnnotationContainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manno_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m dataframe = pd.DataFrame(data=container)\n\u001b[32m    104\u001b[39m dataframe[\u001b[33m\"\u001b[39m\u001b[33msubset\u001b[39m\u001b[33m\"\u001b[39m] = subset\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mAnnotationContainer.__init__\u001b[39m\u001b[34m(self, base_dir, anno_file_path, parse_kps)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(anno_file_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tf:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mself\u001b[39m.content = tf.read()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mAnnotationContainer.__parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m coords = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, point_set.strip().split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Box is a first 4 coordinates in top_left_x, top_left_y, bottom_right_x, bottom_right_y format.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m box = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, coords[:\u001b[32m4\u001b[39m]))\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Artificallly add class label to box.\u001b[39;00m\n\u001b[32m     59\u001b[39m box = [\u001b[32m0\u001b[39m, *box]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing as ty\n",
    "from itertools import islice\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batched_custom(iterable, n):\n",
    "    \"\"\"\n",
    "    Batches an iterable into chunks of size n.\n",
    "    Equivalent to itertools.batched in Python 3.12+.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class AnnotationDict(ty.TypedDict):\n",
    "    image: str\n",
    "    boxes: list[int]\n",
    "    key_points: list[list[int]]\n",
    "\n",
    "\n",
    "class AnnotationContainer:\n",
    "    def __init__(self, base_dir: str, anno_file_path: str, parse_kps: bool = False) -> None:\n",
    "        self.parse_kps = parse_kps\n",
    "        self.base_dir = base_dir\n",
    "        with open(anno_file_path, \"r\") as tf:\n",
    "            self.content = tf.read()\n",
    "        self.__parse()\n",
    "\n",
    "    def __parse(self) -> None:\n",
    "        self.meta = []\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        samples_as_text = self.content.split(\"# \")[1:] # We skip 1 element since it's an empty string.\n",
    "        samples_as_text = [sample.strip().split(\"\\n\") for sample in samples_as_text]\n",
    "        for sample in samples_as_text:\n",
    "            image_meta, *boxes_str = sample\n",
    "            \n",
    "            name, height, width = image_meta.split(\" \")\n",
    "            image_path = os.path.join(*name.split(\"/\")) # This should be correct for Windows and Unix.\n",
    "            self.images.append(image_path)\n",
    "            self.meta.append((height, width))\n",
    "\n",
    "            labels = {\"boxes\": [], \"key_points\": []}\n",
    "            for i, point_set in enumerate(boxes_str):\n",
    "                coords = list(map(float, point_set.strip().split(\" \")))\n",
    "\n",
    "                # Box is a first 4 coordinates in top_left_x, top_left_y, bottom_right_x, bottom_right_y format.\n",
    "                box = list(map(int, coords[:4]))\n",
    "\n",
    "                # Artificallly add class label to box.\n",
    "                box = [0, *box]\n",
    "                if any(coord < 0 for coord in box):\n",
    "                    msg = f\"Image {image_path} has box with negative coords: {box}\"\n",
    "                    raise ValueError(msg)\n",
    "                labels[\"boxes\"].append(box)\n",
    "\n",
    "                # Key points are the rest points. It should be 5 in total, 3 components each (x, y, visibility flag).\n",
    "                if self.parse_kps:\n",
    "                    kps = []\n",
    "                    for point in batched_custom(coords[4:], 3):\n",
    "                        if all(coord == -1 for coord in point):\n",
    "                            kps.append([0.0, 0.0, 0.0])\n",
    "                        else:\n",
    "                            point = list(point)\n",
    "                            point[-1] = 1.0\n",
    "                            kps.append(point)\n",
    "                    if len(kps) != 5:\n",
    "                        msg = f\"Image {image_path} has more or less than 5 kps: {kps}\"\n",
    "                        raise ValueError(msg)\n",
    "                    labels[\"key_points\"].append(kps)\n",
    "                    \n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> AnnotationDict:\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        key_points = label[\"key_points\"]\n",
    "        boxes = label[\"boxes\"]\n",
    "        return {\"image\": image, \"boxes\": boxes, \"key_points\": key_points}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for image_path, label in zip(self.images, self.labels):\n",
    "            yield {\"image\": image_path, **label}\n",
    "            \n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"data\", \"widerface\")\n",
    "dataset_df = None\n",
    "for subset in (\"train\", \"val\"):\n",
    "    anno_file_path = os.path.join(DATA_PATH, \"labelv2\", subset, \"labelv2.txt\")\n",
    "    image_dir = os.path.join(DATA_PATH, f\"WIDER_{subset}\", f\"WIDER_{subset}\", \"images\")\n",
    "    container = AnnotationContainer(image_dir, anno_file_path, subset == \"train\")\n",
    "    dataframe = pd.DataFrame(data=container)\n",
    "    dataframe[\"subset\"] = subset\n",
    "    if dataset_df is None:\n",
    "        dataset_df = dataframe\n",
    "    else:\n",
    "        dataset_df = pd.concat((dataset_df, dataframe))\n",
    "\n",
    "assert dataset_df is not None\n",
    "train_df = dataset_df.query(\"subset == 'train'\")\n",
    "train_idx = train_df.sample(frac=0.8).index\n",
    "val_idx = np.setdiff1d(train_df.index, train_idx)\n",
    "train_df.loc[train_idx, \"subset\"] = \"train\"\n",
    "train_df.loc[val_idx, \"subset\"] = \"val\"\n",
    "val_df = dataset_df.query(\"subset == 'val'\")\n",
    "val_df[\"subset\"] = \"test\"\n",
    "dataset_df = pd.concat((train_df, val_df)).reset_index(drop=True)\n",
    "dataset_df.to_csv(\"widerface_main_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/161 [00:08<23:28,  8.81s/it]c:\\Projects\\yunet_pytorch\\.venv\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "100%|██████████| 161/161 [00:16<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': tensor(0.1202), 'map_50': tensor(0.2936), 'map_75': tensor(0.0793), 'map_small': tensor(0.0807), 'map_medium': tensor(0.3921), 'map_large': tensor(0.4399), 'mar_1': tensor(0.0357), 'mar_10': tensor(0.1108), 'mar_100': tensor(0.1448), 'mar_small': tensor(0.0989), 'mar_medium': tensor(0.4388), 'mar_large': tensor(0.4650), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(0, dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "from train import read_config\n",
    "from source.postprocessing import postprocess_predictions\n",
    "from source.targets import generate_targets_batch\n",
    "from source.dataset import build_dataloaders\n",
    "from source.models.yunet import YuNet\n",
    "\n",
    "\n",
    "def decode_keypoints(kps_preds: torch.Tensor, grids: torch.Tensor) -> torch.Tensor:\n",
    "    num_points = kps_preds.shape[-1] // 2\n",
    "    decoded = []\n",
    "    for i in range(num_points):\n",
    "        kp_encoded = kps_preds[:, [2 * i, 2 * i + 1]]\n",
    "        kp_decoded = kp_encoded * grids[:, 2:] + grids[:, :2]\n",
    "        decoded.append(kp_decoded)\n",
    "    return torch.cat(decoded, dim=1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_map_torchmetrics(\n",
    "    model: YuNet,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    conf_thresh: float = 0.5,\n",
    "    iou_thresh: float = 0.45,\n",
    "    metric_names: tuple[str, ...] = (\"map_50\", \"map_small\", \"map_medium\", \"map_large\"),\n",
    ") -> dict[str, float]:\n",
    "    model.eval()\n",
    "    map_calculator = MeanAveragePrecision(backend=\"faster_coco_eval\").to(device)\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        gt_boxes = [box.to(device, non_blocking=True) for box in batch[\"boxes\"]]\n",
    "        gt_kps = [kp_set.to(device, non_blocking=True) for kp_set in batch[\"key_points\"]]\n",
    "        gt_labels = [bl.to(device, non_blocking=True) for bl in batch[\"box_labels\"]]\n",
    "        p8_out, p16_out, p32_out = model(images)\n",
    "        obj_preds, cls_preds, box_preds, kp_preds, priors = postprocess_predictions(\n",
    "            (p8_out, p16_out, p32_out), (8, 16, 32)\n",
    "        )\n",
    "        batch_size = images.shape[0]\n",
    "        conf = (obj_preds.sigmoid() * cls_preds.squeeze(dim=-1).sigmoid()).sqrt()\n",
    "        prep = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            sample_conf = conf[batch_idx]\n",
    "            sample_boxes = box_preds[batch_idx]\n",
    "            sample_kps = kp_preds[batch_idx]\n",
    "            sample_priors = priors[batch_idx]\n",
    "            decoded_kps = decode_keypoints(sample_kps, sample_priors)\n",
    "\n",
    "            sample_keep = sample_conf >= conf_thresh\n",
    "            filt_conf = sample_conf[sample_keep]\n",
    "            filt_boxes = sample_boxes[sample_keep]\n",
    "            filt_kps = decoded_kps[sample_keep]\n",
    "\n",
    "            keep_indices = nms(filt_boxes, filt_conf, iou_thresh)\n",
    "            filt_conf = filt_conf[keep_indices]\n",
    "            filt_boxes = filt_boxes[keep_indices]\n",
    "            filt_kps = filt_kps[keep_indices]\n",
    "\n",
    "            prep.append(\n",
    "                {\n",
    "                    \"boxes\": filt_boxes,\n",
    "                    \"scores\": filt_conf.view(-1),\n",
    "                    \"labels\": torch.zeros(len(filt_boxes), dtype=torch.long, device=device).view(-1),\n",
    "                }\n",
    "            )\n",
    "        targ = [{\"boxes\": gt_boxes[i], \"labels\": gt_labels[i].view(-1).long()} for i in range(batch_size)]\n",
    "        map_calculator.update(prep, targ)\n",
    "    metrics = {name: tensor.detach().cpu().item() for name, tensor in map_calculator.compute().items()}\n",
    "    return {name: value for name, value in metrics.items() if name in metric_names}\n",
    "\n",
    "\n",
    "config = read_config(\"config.yml\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "dataframe = pd.read_csv(config.path.csv)\n",
    "dataloaders = build_dataloaders(config, dataframe)\n",
    "model = YuNet(**config.model.model_dump()).to(device)\n",
    "ckpt = torch.load(os.path.join(\"artifacts\", \"objective_blackburn\", \"checkpoints\", \"epoch_112_ckpt.pt\"))\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "map_calculator = MeanAveragePrecision(backend=\"faster_coco_eval\").to(device)\n",
    "pbar = tqdm(dataloaders[\"val\"], total=len(dataloaders[\"val\"]))\n",
    "with torch.no_grad():\n",
    "    for batch in pbar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        gt_boxes = [box.to(device) for box in batch[\"boxes\"]]\n",
    "        gt_kps = [kp_set.to(device) for kp_set in batch[\"key_points\"]]\n",
    "        gt_labels = [bl.to(device) for bl in batch[\"box_labels\"]]\n",
    "        p8_out, p16_out, p32_out = model(images)\n",
    "        obj_preds, cls_preds, box_preds, kp_preds, priors = postprocess_predictions(\n",
    "            (p8_out, p16_out, p32_out), (8, 16, 32)\n",
    "        )\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        conf = (obj_preds.sigmoid() * cls_preds.squeeze(dim=-1).sigmoid()).sqrt()\n",
    "        prep = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            sample_conf = conf[batch_idx]\n",
    "            sample_boxes = box_preds[batch_idx]\n",
    "            sample_kps = kp_preds[batch_idx]\n",
    "            sample_priors = priors[batch_idx]\n",
    "            decoded_kps = decode_keypoints(sample_kps, sample_priors)\n",
    "\n",
    "            sample_keep = sample_conf >= 0.5\n",
    "            filt_conf = sample_conf[sample_keep]\n",
    "            filt_boxes = sample_boxes[sample_keep]\n",
    "            filt_kps = decoded_kps[sample_keep]\n",
    "\n",
    "            keep_indices = nms(filt_boxes, filt_conf, 0.45)\n",
    "            filt_conf = filt_conf[keep_indices]\n",
    "            filt_boxes = filt_boxes[keep_indices]\n",
    "            filt_kps = filt_kps[keep_indices]\n",
    "\n",
    "            prep.append(\n",
    "                {\n",
    "                    \"boxes\": filt_boxes,\n",
    "                    \"scores\": filt_conf.view(-1),\n",
    "                    \"labels\": torch.zeros(len(filt_boxes), dtype=torch.long, device=device).view(-1),\n",
    "                }\n",
    "            )\n",
    "        targ = [{\"boxes\": gt_boxes[i], \"labels\": gt_labels[i].view(-1).long()} for i in range(batch_size)]\n",
    "        map_calculator.update(prep, targ)\n",
    "    _map = map_calculator.compute()\n",
    "    print(_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
