{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce80c86-76c4-4344-9a01-758fe50aee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>boxes</th>\n",
       "      <th>key_points</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_849.jpg</td>\n",
       "      <td>[[449, 330, 571, 479]]</td>\n",
       "      <td>[[[488.90601, 373.64301], [542.08899, 376.4419...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0--Parade\\0_Parade_Parade_0_904.jpg</td>\n",
       "      <td>[[361, 98, 624, 437]]</td>\n",
       "      <td>[[[424.14301, 251.65601], [547.13397, 232.571]...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_799.jpg</td>\n",
       "      <td>[[78, 221, 85, 229], [78, 238, 92, 255], [113,...</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_117.jpg</td>\n",
       "      <td>[[69, 359, 119, 395], [227, 382, 283, 425], [2...</td>\n",
       "      <td>[[[92.232, 391.397], [94.451, 377.45099], [103...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_778.jpg</td>\n",
       "      <td>[[27, 226, 60, 262], [63, 95, 79, 114], [64, 6...</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       image  \\\n",
       "0  0--Parade\\0_Parade_marchingband_1_849.jpg   \n",
       "1        0--Parade\\0_Parade_Parade_0_904.jpg   \n",
       "2  0--Parade\\0_Parade_marchingband_1_799.jpg   \n",
       "3  0--Parade\\0_Parade_marchingband_1_117.jpg   \n",
       "4  0--Parade\\0_Parade_marchingband_1_778.jpg   \n",
       "\n",
       "                                               boxes  \\\n",
       "0                             [[449, 330, 571, 479]]   \n",
       "1                              [[361, 98, 624, 437]]   \n",
       "2  [[78, 221, 85, 229], [78, 238, 92, 255], [113,...   \n",
       "3  [[69, 359, 119, 395], [227, 382, 283, 425], [2...   \n",
       "4  [[27, 226, 60, 262], [63, 95, 79, 114], [64, 6...   \n",
       "\n",
       "                                          key_points subset  \n",
       "0  [[[488.90601, 373.64301], [542.08899, 376.4419...  train  \n",
       "1  [[[424.14301, 251.65601], [547.13397, 232.571]...  train  \n",
       "2  [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...  train  \n",
       "3  [[[92.232, 391.397], [94.451, 377.45099], [103...  train  \n",
       "4  [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...  train  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import typing as ty\n",
    "from itertools import islice\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batched_custom(iterable, n):\n",
    "    \"\"\"\n",
    "    Batches an iterable into chunks of size n.\n",
    "    Equivalent to itertools.batched in Python 3.12+.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class AnnotationDict(ty.TypedDict):\n",
    "    image: str\n",
    "    boxes: list[int]\n",
    "    key_points: list[list[int]]\n",
    "\n",
    "\n",
    "class AnnotationContainer:\n",
    "    def __init__(self, base_dir: str, anno_file_path: str, parse_kps: bool = False) -> None:\n",
    "        self.parse_kps = parse_kps\n",
    "        self.base_dir = base_dir\n",
    "        with open(anno_file_path, \"r\") as tf:\n",
    "            self.content = tf.read()\n",
    "        self.__parse()\n",
    "\n",
    "    def __parse(self) -> None:\n",
    "        self.meta = []\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        samples_as_text = self.content.split(\"# \")[1:] # We skip 1 element since it's an empty string.\n",
    "        samples_as_text = [sample.strip().split(\"\\n\") for sample in samples_as_text]\n",
    "        for sample in samples_as_text:\n",
    "            image_meta, *boxes_str = sample\n",
    "            \n",
    "            name, height, width = image_meta.split(\" \")\n",
    "            image_path = os.path.join(*name.split(\"/\")) # This should be correct for Windows and Unix.\n",
    "            self.images.append(image_path)\n",
    "            self.meta.append((height, width))\n",
    "\n",
    "            labels = {\"boxes\": [], \"key_points\": []}\n",
    "            for i, point_set in enumerate(boxes_str):\n",
    "                coords = list(map(float, point_set.strip().split(\" \")))\n",
    "\n",
    "                # Box is a first 4 coordinates in top_left_x, top_left_y, bottom_right_x, bottom_right_y format.\n",
    "                box = list(map(int, coords[:4]))\n",
    "                if any(coord < 0 for coord in box):\n",
    "                    msg = f\"Image {image_path} has box with negative coords: {box}\"\n",
    "                    raise ValueError(msg)\n",
    "                labels[\"boxes\"].append(box)\n",
    "\n",
    "                # Key points are the rest points. It should be 5 in total, 3 components each (x, y, ...). I don't know\n",
    "                # what is 3rd component.\n",
    "                if self.parse_kps:\n",
    "                    kps = []\n",
    "                    for point in batched_custom(coords[4:], 3):\n",
    "                        kps.append(list(point[:2]))\n",
    "                    if len(kps) != 5:\n",
    "                        msg = f\"Image {image_path} has more or less than 5 kps: {kps}\"\n",
    "                        raise ValueError(msg)\n",
    "                    labels[\"key_points\"].append(kps)\n",
    "                    \n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> AnnotationDict:\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        key_points = label[\"key_points\"]\n",
    "        return {\"image\": image, \"boxes\": boxes, \"key_points\": key_points}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for image_path, label in zip(self.images, self.labels):\n",
    "            yield {\"image\": image_path, **label}\n",
    "            \n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"data\", \"widerface\")\n",
    "dataset_df = None\n",
    "for subset in (\"train\", \"val\"):\n",
    "    anno_file_path = os.path.join(DATA_PATH, \"labelv2\", subset, \"labelv2.txt\")\n",
    "    image_dir = os.path.join(DATA_PATH, f\"WIDER_{subset}\", f\"WIDER_{subset}\", \"images\")\n",
    "    container = AnnotationContainer(image_dir, anno_file_path, subset == \"train\")\n",
    "    dataframe = pd.DataFrame(data=container)\n",
    "    dataframe[\"subset\"] = subset\n",
    "    if dataset_df is None:\n",
    "        dataset_df = dataframe\n",
    "    else:\n",
    "        dataset_df = pd.concat((dataset_df, dataframe))\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84503a80-6754-40cb-8219-9b8020c00cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[[488.90601, 373.64301], [542.08899, 376.4419...\n",
       "1       [[[424.14301, 251.65601], [547.13397, 232.571]...\n",
       "2       [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...\n",
       "3       [[[92.232, 391.397], [94.451, 377.45099], [103...\n",
       "4       [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...\n",
       "                              ...                        \n",
       "3221                                                   []\n",
       "3222                                                   []\n",
       "3223                                                   []\n",
       "3224                                                   []\n",
       "3225                                                   []\n",
       "Name: key_points, Length: 16106, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[\"key_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1849eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>boxes</th>\n",
       "      <th>key_points</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_849.jpg</td>\n",
       "      <td>[[449, 330, 571, 479]]</td>\n",
       "      <td>[[[488.90601, 373.64301], [542.08899, 376.4419...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0--Parade\\0_Parade_Parade_0_904.jpg</td>\n",
       "      <td>[[361, 98, 624, 437]]</td>\n",
       "      <td>[[[424.14301, 251.65601], [547.13397, 232.571]...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_799.jpg</td>\n",
       "      <td>[[78, 221, 85, 229], [78, 238, 92, 255], [113,...</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_117.jpg</td>\n",
       "      <td>[[69, 359, 119, 395], [227, 382, 283, 425], [2...</td>\n",
       "      <td>[[[92.232, 391.397], [94.451, 377.45099], [103...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_778.jpg</td>\n",
       "      <td>[[27, 226, 60, 262], [63, 95, 79, 114], [64, 6...</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[334, 182, 634, 582]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[316, 224, 586, 571]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[332, 172, 626, 544]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[336, 242, 488, 444], [712, 278, 838, 430]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[218, 190, 330, 350], [302, 224, 376, 364], [...</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16106 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  \\\n",
       "0             0--Parade\\0_Parade_marchingband_1_849.jpg   \n",
       "1                   0--Parade\\0_Parade_Parade_0_904.jpg   \n",
       "2             0--Parade\\0_Parade_marchingband_1_799.jpg   \n",
       "3             0--Parade\\0_Parade_marchingband_1_117.jpg   \n",
       "4             0--Parade\\0_Parade_marchingband_1_778.jpg   \n",
       "...                                                 ...   \n",
       "3221  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3222  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3223  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3224  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3225  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "\n",
       "                                                  boxes  \\\n",
       "0                                [[449, 330, 571, 479]]   \n",
       "1                                 [[361, 98, 624, 437]]   \n",
       "2     [[78, 221, 85, 229], [78, 238, 92, 255], [113,...   \n",
       "3     [[69, 359, 119, 395], [227, 382, 283, 425], [2...   \n",
       "4     [[27, 226, 60, 262], [63, 95, 79, 114], [64, 6...   \n",
       "...                                                 ...   \n",
       "3221                             [[334, 182, 634, 582]]   \n",
       "3222                             [[316, 224, 586, 571]]   \n",
       "3223                             [[332, 172, 626, 544]]   \n",
       "3224       [[336, 242, 488, 444], [712, 278, 838, 430]]   \n",
       "3225  [[218, 190, 330, 350], [302, 224, 376, 364], [...   \n",
       "\n",
       "                                             key_points subset  \n",
       "0     [[[488.90601, 373.64301], [542.08899, 376.4419...  train  \n",
       "1     [[[424.14301, 251.65601], [547.13397, 232.571]...  train  \n",
       "2     [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...  train  \n",
       "3     [[[92.232, 391.397], [94.451, 377.45099], [103...  train  \n",
       "4     [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...  train  \n",
       "...                                                 ...    ...  \n",
       "3221                                                 []    val  \n",
       "3222                                                 []    val  \n",
       "3223                                                 []    val  \n",
       "3224                                                 []    val  \n",
       "3225                                                 []    val  \n",
       "\n",
       "[16106 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9dbeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>boxes</th>\n",
       "      <th>key_points</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_849.jpg</td>\n",
       "      <td>[[449, 330, 571, 479]]</td>\n",
       "      <td>[[[488.90601, 373.64301], [542.08899, 376.4419...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0--Parade\\0_Parade_Parade_0_904.jpg</td>\n",
       "      <td>[[361, 98, 624, 437]]</td>\n",
       "      <td>[[[424.14301, 251.65601], [547.13397, 232.571]...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_799.jpg</td>\n",
       "      <td>[[78, 221, 85, 229], [78, 238, 92, 255], [113,...</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_117.jpg</td>\n",
       "      <td>[[69, 359, 119, 395], [227, 382, 283, 425], [2...</td>\n",
       "      <td>[[[92.232, 391.397], [94.451, 377.45099], [103...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0--Parade\\0_Parade_marchingband_1_778.jpg</td>\n",
       "      <td>[[27, 226, 60, 262], [63, 95, 79, 114], [64, 6...</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[334, 182, 634, 582]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[316, 224, 586, 571]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[332, 172, 626, 544]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[336, 242, 488, 444], [712, 278, 838, 430]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>9--Press_Conference\\9_Press_Conference_Press_C...</td>\n",
       "      <td>[[218, 190, 330, 350], [302, 224, 376, 364], [...</td>\n",
       "      <td>[]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16106 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  \\\n",
       "0             0--Parade\\0_Parade_marchingband_1_849.jpg   \n",
       "1                   0--Parade\\0_Parade_Parade_0_904.jpg   \n",
       "2             0--Parade\\0_Parade_marchingband_1_799.jpg   \n",
       "3             0--Parade\\0_Parade_marchingband_1_117.jpg   \n",
       "4             0--Parade\\0_Parade_marchingband_1_778.jpg   \n",
       "...                                                 ...   \n",
       "3221  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3222  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3223  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3224  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "3225  9--Press_Conference\\9_Press_Conference_Press_C...   \n",
       "\n",
       "                                                  boxes  \\\n",
       "0                                [[449, 330, 571, 479]]   \n",
       "1                                 [[361, 98, 624, 437]]   \n",
       "2     [[78, 221, 85, 229], [78, 238, 92, 255], [113,...   \n",
       "3     [[69, 359, 119, 395], [227, 382, 283, 425], [2...   \n",
       "4     [[27, 226, 60, 262], [63, 95, 79, 114], [64, 6...   \n",
       "...                                                 ...   \n",
       "3221                             [[334, 182, 634, 582]]   \n",
       "3222                             [[316, 224, 586, 571]]   \n",
       "3223                             [[332, 172, 626, 544]]   \n",
       "3224       [[336, 242, 488, 444], [712, 278, 838, 430]]   \n",
       "3225  [[218, 190, 330, 350], [302, 224, 376, 364], [...   \n",
       "\n",
       "                                             key_points subset  \n",
       "0     [[[488.90601, 373.64301], [542.08899, 376.4419...  train  \n",
       "1     [[[424.14301, 251.65601], [547.13397, 232.571]...  train  \n",
       "2     [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...  train  \n",
       "3     [[[92.232, 391.397], [94.451, 377.45099], [103...  train  \n",
       "4     [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [-...  train  \n",
       "...                                                 ...    ...  \n",
       "3221                                                 []    val  \n",
       "3222                                                 []    val  \n",
       "3223                                                 []    val  \n",
       "3224                                                 []    val  \n",
       "3225                                                 []    val  \n",
       "\n",
       "[16106 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7564cbf-4d1f-4d25-b84c-6b28a02fc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df[dataset_df[\"boxes\"].apply(len) == 0]\n",
    "dataset_df.to_csv(\"widerface_main_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\yunet_pytorch\\.venv\\Lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4319.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 1/640] train_total_loss=3.4931, train_obj_loss=0.2484, train_cls_loss=0.3117, train_box_loss=2.8238, train_kps_loss=0.1093|mAP@0.45=0.0008, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 2/640] train_total_loss=2.2554, train_obj_loss=0.0293, train_cls_loss=0.0325, train_box_loss=2.1067, train_kps_loss=0.0869|mAP@0.45=0.0154, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 3/640] train_total_loss=1.9599, train_obj_loss=0.0219, train_cls_loss=0.0032, train_box_loss=1.8539, train_kps_loss=0.0809|mAP@0.45=0.1027, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 4/640] train_total_loss=1.7950, train_obj_loss=0.0206, train_cls_loss=0.0011, train_box_loss=1.6985, train_kps_loss=0.0748|mAP@0.45=0.1468, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 5/640] train_total_loss=1.6904, train_obj_loss=0.0195, train_cls_loss=0.0007, train_box_loss=1.5989, train_kps_loss=0.0714|mAP@0.45=0.2082, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 6/640] train_total_loss=1.6181, train_obj_loss=0.0187, train_cls_loss=0.0006, train_box_loss=1.5294, train_kps_loss=0.0694|mAP@0.45=0.2682, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 7/640] train_total_loss=1.5798, train_obj_loss=0.0177, train_cls_loss=0.0005, train_box_loss=1.4942, train_kps_loss=0.0673|mAP@0.45=0.2354, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 8/640] train_total_loss=1.5329, train_obj_loss=0.0173, train_cls_loss=0.0005, train_box_loss=1.4488, train_kps_loss=0.0663|mAP@0.45=0.3105, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 9/640] train_total_loss=1.5157, train_obj_loss=0.0166, train_cls_loss=0.0004, train_box_loss=1.4336, train_kps_loss=0.0650|mAP@0.45=0.3397, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 10/640] train_total_loss=1.4814, train_obj_loss=0.0164, train_cls_loss=0.0004, train_box_loss=1.4004, train_kps_loss=0.0642|mAP@0.45=0.3722, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 11/640] train_total_loss=1.4634, train_obj_loss=0.0160, train_cls_loss=0.0004, train_box_loss=1.3834, train_kps_loss=0.0636|mAP@0.45=0.3808, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 12/640] train_total_loss=1.4417, train_obj_loss=0.0158, train_cls_loss=0.0004, train_box_loss=1.3633, train_kps_loss=0.0622|mAP@0.45=0.3810, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 13/640] train_total_loss=1.4246, train_obj_loss=0.0156, train_cls_loss=0.0004, train_box_loss=1.3464, train_kps_loss=0.0622|mAP@0.45=0.3966, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 14/640] train_total_loss=1.4168, train_obj_loss=0.0153, train_cls_loss=0.0004, train_box_loss=1.3391, train_kps_loss=0.0620|mAP@0.45=0.4189, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 15/640] train_total_loss=1.4000, train_obj_loss=0.0151, train_cls_loss=0.0003, train_box_loss=1.3231, train_kps_loss=0.0613|mAP@0.45=0.4365, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 16/640] train_total_loss=1.3852, train_obj_loss=0.0150, train_cls_loss=0.0003, train_box_loss=1.3089, train_kps_loss=0.0609|mAP@0.45=0.4366, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 17/640] train_total_loss=1.3821, train_obj_loss=0.0148, train_cls_loss=0.0003, train_box_loss=1.3066, train_kps_loss=0.0604|mAP@0.45=0.4068, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 18/640] train_total_loss=1.3691, train_obj_loss=0.0147, train_cls_loss=0.0003, train_box_loss=1.2943, train_kps_loss=0.0598|mAP@0.45=0.4446, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 19/640] train_total_loss=1.3524, train_obj_loss=0.0145, train_cls_loss=0.0003, train_box_loss=1.2772, train_kps_loss=0.0603|mAP@0.45=0.4470, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 20/640] train_total_loss=1.3520, train_obj_loss=0.0143, train_cls_loss=0.0003, train_box_loss=1.2781, train_kps_loss=0.0593|mAP@0.45=0.4355, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 21/640] train_total_loss=1.3539, train_obj_loss=0.0143, train_cls_loss=0.0003, train_box_loss=1.2795, train_kps_loss=0.0598|mAP@0.45=0.4584, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 22/640] train_total_loss=1.3425, train_obj_loss=0.0142, train_cls_loss=0.0003, train_box_loss=1.2684, train_kps_loss=0.0595|mAP@0.45=0.4739, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 23/640] train_total_loss=1.3447, train_obj_loss=0.0141, train_cls_loss=0.0003, train_box_loss=1.2711, train_kps_loss=0.0592|mAP@0.45=0.4852, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 24/640] train_total_loss=1.3241, train_obj_loss=0.0141, train_cls_loss=0.0003, train_box_loss=1.2505, train_kps_loss=0.0592|mAP@0.45=0.4477, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 25/640] train_total_loss=1.3338, train_obj_loss=0.0140, train_cls_loss=0.0003, train_box_loss=1.2605, train_kps_loss=0.0589|mAP@0.45=0.4794, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 26/640] train_total_loss=1.2985, train_obj_loss=0.0139, train_cls_loss=0.0003, train_box_loss=1.2258, train_kps_loss=0.0585|mAP@0.45=0.4380, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 27/640] train_total_loss=1.3077, train_obj_loss=0.0138, train_cls_loss=0.0003, train_box_loss=1.2352, train_kps_loss=0.0585|mAP@0.45=0.4668, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 28/640] train_total_loss=1.3238, train_obj_loss=0.0138, train_cls_loss=0.0003, train_box_loss=1.2513, train_kps_loss=0.0585|mAP@0.45=0.5073, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 29/640] train_total_loss=1.3004, train_obj_loss=0.0136, train_cls_loss=0.0003, train_box_loss=1.2280, train_kps_loss=0.0584|mAP@0.45=0.4587, NME=0.0000\n",
      "[VAL DEBUG] faces_used_for_NME=0, nme_count=0\n",
      "[EPOCH 30/640] train_total_loss=1.2969, train_obj_loss=0.0137, train_cls_loss=0.0003, train_box_loss=1.2244, train_kps_loss=0.0584|mAP@0.45=0.5027, NME=0.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from source.postprocessing import postprocess_predictions\n",
    "from source.targets import generate_targets_batch\n",
    "from source.dataset import build_dataloaders\n",
    "from source.general import get_cpu_state_dict\n",
    "from source.losses import DetectionLoss\n",
    "from source.models.yunet import YuNet\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.ops import box_iou, nms\n",
    "\n",
    "from source.postprocessing import postprocess_predictions\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    device: torch.device,\n",
    "    iou_thr: float = 0.45,\n",
    "    score_thr: float = 0.0,  # для mAP лучше не резать по score, максимум очень низкий порог\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Валидация:\n",
    "      - mAP@iou_thr по боксам (один класс \"face\");\n",
    "      - NME по 5 кейпоинтам.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Для mAP: собираем все детекции и GT по всему датасету\n",
    "    all_scores = []      # list[float]\n",
    "    all_tp_flags = []    # list[int] 1/0 для каждой детекции\n",
    "    num_gt_total = 0\n",
    "\n",
    "    # Для NME\n",
    "    nme_sum = 0.0\n",
    "    nme_count = 0\n",
    "    debug_faces = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        gt_boxes_batch = batch[\"boxes\"]          # list[Tensor (Mi, 4)]\n",
    "        gt_kps_batch = batch[\"key_points\"]       # list[Tensor (Mi, 5, 2)]\n",
    "\n",
    "        p8_out, p16_out, p32_out = model(images)\n",
    "        obj_logits, cls_logits, boxes_xyxy, kps_logits, priors = postprocess_predictions(\n",
    "            (p8_out, p16_out, p32_out),\n",
    "            (8, 16, 32),\n",
    "        )\n",
    "\n",
    "        obj_probs = obj_logits.sigmoid()         # (B, N)\n",
    "        cls_probs = cls_logits.sigmoid()[..., 0] # (B, N)\n",
    "        scores = obj_probs * cls_probs           # (B, N)\n",
    "\n",
    "        B = images.shape[0]\n",
    "        for b in range(B):\n",
    "            gt_boxes = gt_boxes_batch[b].to(device).float()      # (M, 4)\n",
    "            gt_kps = gt_kps_batch[b].to(device).float()          # (M, 5, 2)\n",
    "            num_gt = gt_boxes.shape[0]\n",
    "            num_gt_total += num_gt\n",
    "\n",
    "            scores_b = scores[b]         # (N,)\n",
    "            boxes_b = boxes_xyxy[b]      # (N, 4)\n",
    "            kps_b = kps_logits[b]        # (N, 10)\n",
    "            priors_b = priors[b]         # (N, 4)\n",
    "\n",
    "            # очень мягкий порог по score, чисто чтобы выкинуть совсем мусор\n",
    "            keep = scores_b >= score_thr\n",
    "            if keep.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            scores_b = scores_b[keep]\n",
    "            boxes_b = boxes_b[keep]\n",
    "            kps_b = kps_b[keep]\n",
    "            priors_b = priors_b[keep]\n",
    "\n",
    "            # NMS\n",
    "            keep_nms = nms(boxes_b, scores_b, iou_thr)\n",
    "            scores_b = scores_b[keep_nms]\n",
    "            boxes_b = boxes_b[keep_nms]\n",
    "            kps_b = kps_b[keep_nms]\n",
    "            priors_b = priors_b[keep_nms]\n",
    "\n",
    "            num_dets = boxes_b.shape[0]\n",
    "            if num_dets == 0:\n",
    "                continue\n",
    "\n",
    "            # ===== mAP: TP/FP для каждой детекции =====\n",
    "            if num_gt == 0:\n",
    "                # все детекции — FP\n",
    "                all_scores.extend(scores_b.tolist())\n",
    "                all_tp_flags.extend([0] * num_dets)\n",
    "            else:\n",
    "                ious = box_iou(boxes_b, gt_boxes)  # (Ndets, Ngt)\n",
    "                # чтобы одна GT не матчилась много раз\n",
    "                gt_matched = torch.zeros(num_gt, dtype=torch.bool, device=device)\n",
    "\n",
    "                for d in range(num_dets):\n",
    "                    iou_row = ious[d]\n",
    "                    max_iou, gi = iou_row.max(dim=0)\n",
    "                    gi = gi.item()\n",
    "                    if max_iou >= iou_thr and not gt_matched[gi]:\n",
    "                        all_scores.append(float(scores_b[d].item()))\n",
    "                        all_tp_flags.append(1)\n",
    "                        gt_matched[gi] = True\n",
    "                    else:\n",
    "                        all_scores.append(float(scores_b[d].item()))\n",
    "                        all_tp_flags.append(0)\n",
    "\n",
    "            # ===== NME по кейпоинтам =====\n",
    "            if num_gt == 0 or num_dets == 0 or gt_kps.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # декодируем keypoints из нормализованных в пиксели\n",
    "            num_points = kps_b.shape[1] // 2  # 5\n",
    "            kps_abs_list = []\n",
    "            for i in range(num_points):\n",
    "                kp_xy = kps_b[:, [2 * i, 2 * i + 1]]          # (Ndets, 2)\n",
    "                kp_xy_abs = kp_xy * priors_b[:, 2:] + priors_b[:, :2]\n",
    "                kps_abs_list.append(kp_xy_abs)\n",
    "            kps_abs = torch.stack(kps_abs_list, dim=1)        # (Ndets, 5, 2)\n",
    "\n",
    "            # матчинг для NME: для каждого GT берём лучшую детекцию по IoU\n",
    "            ious_kps = box_iou(boxes_b, gt_boxes)      # (Ndets, Ngt)\n",
    "            max_det_iou, det_idx_for_gt = ious_kps.max(dim=0)  # (Ngt,)\n",
    "\n",
    "            for gi in range(num_gt):\n",
    "                if max_det_iou[gi] < iou_thr:\n",
    "                    continue\n",
    "\n",
    "                d = det_idx_for_gt[gi].item()\n",
    "                gt_kps_face = gt_kps[gi]          # (5, 2)\n",
    "                det_kps_face = kps_abs[d]         # (5, 2)\n",
    "\n",
    "                # если в GT есть -1 (нет валидных kps) — пропускаем\n",
    "                if (gt_kps_face == -1).any():\n",
    "                    continue\n",
    "\n",
    "                # нормирующий размер: средняя сторона GT-бокса\n",
    "                w = gt_boxes[gi, 2] - gt_boxes[gi, 0]\n",
    "                h = gt_boxes[gi, 3] - gt_boxes[gi, 1]\n",
    "                face_size = (w + h) / 2.0\n",
    "                if face_size <= 0:\n",
    "                    continue\n",
    "\n",
    "                per_point_err = (det_kps_face - gt_kps_face).norm(dim=-1)  # (5,)\n",
    "                nme_face = per_point_err.mean() / face_size                # скаляр\n",
    "\n",
    "                nme_sum += float(nme_face)\n",
    "                nme_count += 1\n",
    "                debug_faces += 1\n",
    "\n",
    "    # ===== mAP расчёт =====\n",
    "    if len(all_scores) == 0 or num_gt_total == 0:\n",
    "        ap = 0.0\n",
    "    else:\n",
    "        scores_t = torch.tensor(all_scores)\n",
    "        tps = torch.tensor(all_tp_flags).float()\n",
    "\n",
    "        # сортировка по score по убыванию\n",
    "        sorted_idx = torch.argsort(scores_t, descending=True)\n",
    "        tps = tps[sorted_idx]\n",
    "\n",
    "        fps = 1.0 - tps\n",
    "        tp_cum = torch.cumsum(tps, dim=0)\n",
    "        fp_cum = torch.cumsum(fps, dim=0)\n",
    "\n",
    "        recalls = tp_cum / max(1, num_gt_total)              # R(k)\n",
    "        precisions = tp_cum / torch.clamp(tp_cum + fp_cum, min=1e-8)  # P(k)\n",
    "\n",
    "        # классический вычислитель AP по PR-кривой (интерполяция по 11 точкам можно не делать)\n",
    "        # делаем интеграл по ломаной: sum (R_i - R_{i-1}) * P_i\n",
    "        # сначала добавим (0,1) точку\n",
    "        recalls = torch.cat([torch.tensor([0.0]), recalls])\n",
    "        precisions = torch.cat([torch.tensor([1.0]), precisions])\n",
    "\n",
    "        # сглаживаем precision как в VOC: P_interp(r) = max_{r' >= r} P(r')\n",
    "        for i in range(precisions.shape[0] - 2, -1, -1):\n",
    "            precisions[i] = torch.maximum(precisions[i], precisions[i + 1])\n",
    "\n",
    "        # AP = сумма площадей под кривой\n",
    "        ap = torch.sum((recalls[1:] - recalls[:-1]) * precisions[1:]).item()\n",
    "\n",
    "    nme = nme_sum / max(1, nme_count)\n",
    "    print(f\"[VAL DEBUG] faces_used_for_NME={debug_faces}, nme_count={nme_count}\")\n",
    "\n",
    "    return {\n",
    "        f\"mAP@{iou_thr:.2f}\": ap,\n",
    "        \"NME\": nme,\n",
    "    }\n",
    "\n",
    "\n",
    "def lr_lambda(current_iter):\n",
    "    if current_iter >= warmup_iters:\n",
    "        return 1.0\n",
    "    # linear warmup от warmup_ratio до 1.0\n",
    "    alpha = current_iter / float(warmup_iters)\n",
    "    return warmup_ratio * (1 - alpha) + alpha\n",
    "\n",
    "\n",
    "def nan_hook(name):\n",
    "    def hook(module, inp, out):\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            outs = out\n",
    "        else:\n",
    "            outs = (out,)\n",
    "        for o in outs:\n",
    "            if torch.isnan(o).any() or torch.isinf(o).any():\n",
    "                print(f\"NaN in module {name}\")\n",
    "                raise RuntimeError(f\"NaN detected after {name}\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "num_epochs = 80 * 8  # 640\n",
    "milestones = [50 * 8, 68 * 8]  # [400, 544]\n",
    "warmup_iters = 1500\n",
    "warmup_ratio = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "yunet = YuNet().to(device)\n",
    "handles = []\n",
    "for name, module in yunet.named_modules():\n",
    "    if len(list(module.children())) == 0:  # только \"листья\"\n",
    "        handles.append(module.register_forward_hook(nan_hook(name)))\n",
    "\n",
    "dataloaders = build_dataloaders(DATA_PATH, dataset_df, torch.device(\"cpu\"))\n",
    "optimizer = torch.optim.SGD(yunet.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "base_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1,)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "criterion = DetectionLoss(obj_weight=1.0, cls_weight=1.0, box_weight=5.0, kps_weight=0.1)\n",
    "\n",
    "global_iter = 0\n",
    "train_dataloader = dataloaders[\"train\"]\n",
    "for epoch in range(num_epochs):\n",
    "    running_losses: defaultdict[str, float] = defaultdict(float)\n",
    "    yunet.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        boxes = [item.to(device, non_blocking=True) for item in batch[\"boxes\"]]\n",
    "        kps = [item.to(device, non_blocking=True) for item in batch[\"key_points\"]]\n",
    "        p8_out, p16_out, p32_out = yunet(images)\n",
    "        obj_preds, cls_preds, box_preds, kps_preds, grids = postprocess_predictions((p8_out, p16_out, p32_out), (8, 16, 32))\n",
    "        foreground_mask, target_cls, target_obj, target_boxes, target_kps = generate_targets_batch(obj_preds, cls_preds, box_preds, grids, boxes, kps, device)\n",
    "        loss_dict: dict[str, torch.Tensor] = criterion(\n",
    "            (obj_preds, cls_preds, box_preds, kps_preds),\n",
    "            (target_obj, target_cls, target_boxes, target_kps),\n",
    "            foreground_mask,\n",
    "            grids,\n",
    "        )\n",
    "        loss = loss_dict[\"total_loss\"]\n",
    "        loss.backward()\n",
    "        # # Клиппинг по норме (рекомендуемо для твоего случая)\n",
    "        # max_norm = 100\n",
    "        # torch.nn.utils.clip_grad_norm_(yunet.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        if global_iter < warmup_iters:\n",
    "            warmup_scheduler.step()\n",
    "        global_iter += 1\n",
    "\n",
    "        for loss_name, loss_tensor in loss_dict.items():\n",
    "            loss_value = loss_tensor.detach().cpu().item()\n",
    "            running_losses[f\"train_{loss_name}\"] += loss_value / len(train_dataloader)\n",
    "\n",
    "    val_results = validate(yunet, dataloaders[\"val\"], device, score_thr=0.02, iou_thr=0.45)\n",
    "\n",
    "    base_scheduler.step()\n",
    "    loss_str = \", \".join([f\"{loss_name}={loss_value:.4f}\" for loss_name, loss_value in running_losses.items()])\n",
    "    loss_str += \"|\" + \", \".join([f\"{name}={val:.4f}\" for name, val in val_results.items()])\n",
    "    print(f\"[EPOCH {epoch + 1}/{num_epochs}] {loss_str}\")\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": get_cpu_state_dict(yunet.state_dict()),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"warmup_scheduler\": warmup_scheduler.state_dict(),\n",
    "        \"base_scheduler\": base_scheduler.state_dict(),\n",
    "    }\n",
    "    torch.save(ckpt, f\"weights/epoch_{epoch}_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
