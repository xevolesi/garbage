{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce80c86-76c4-4344-9a01-758fe50aee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\easyt\\AppData\\Local\\Temp\\ipykernel_15800\\2701843046.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df[\"subset\"] = \"test\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing as ty\n",
    "from itertools import islice\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batched_custom(iterable, n):\n",
    "    \"\"\"\n",
    "    Batches an iterable into chunks of size n.\n",
    "    Equivalent to itertools.batched in Python 3.12+.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class AnnotationDict(ty.TypedDict):\n",
    "    image: str\n",
    "    boxes: list[int]\n",
    "    key_points: list[list[int]]\n",
    "\n",
    "\n",
    "class AnnotationContainer:\n",
    "    def __init__(self, base_dir: str, anno_file_path: str, parse_kps: bool = False) -> None:\n",
    "        self.parse_kps = parse_kps\n",
    "        self.base_dir = base_dir\n",
    "        with open(anno_file_path, \"r\") as tf:\n",
    "            self.content = tf.read()\n",
    "        self.__parse()\n",
    "\n",
    "    def __parse(self) -> None:\n",
    "        self.meta = []\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        samples_as_text = self.content.split(\"# \")[1:] # We skip 1 element since it's an empty string.\n",
    "        samples_as_text = [sample.strip().split(\"\\n\") for sample in samples_as_text]\n",
    "        for sample in samples_as_text:\n",
    "            image_meta, *boxes_str = sample\n",
    "            \n",
    "            name, height, width = image_meta.split(\" \")\n",
    "            image_path = os.path.join(*name.split(\"/\")) # This should be correct for Windows and Unix.\n",
    "            self.images.append(image_path)\n",
    "            self.meta.append((height, width))\n",
    "\n",
    "            labels = {\"boxes\": [], \"key_points\": []}\n",
    "            for i, point_set in enumerate(boxes_str):\n",
    "                coords = list(map(float, point_set.strip().split(\" \")))\n",
    "\n",
    "                # Box is a first 4 coordinates in top_left_x, top_left_y, bottom_right_x, bottom_right_y format.\n",
    "                box = list(map(int, coords[:4]))\n",
    "\n",
    "                # Artificallly add class label to box.\n",
    "                box = [0, *box]\n",
    "                if any(coord < 0 for coord in box):\n",
    "                    msg = f\"Image {image_path} has box with negative coords: {box}\"\n",
    "                    raise ValueError(msg)\n",
    "                labels[\"boxes\"].append(box)\n",
    "\n",
    "                # Key points are the rest points. It should be 5 in total, 3 components each (x, y, visibility flag).\n",
    "                if self.parse_kps:\n",
    "                    kps = []\n",
    "                    for point in batched_custom(coords[4:], 3):\n",
    "                        if all(coord == -1 for coord in point):\n",
    "                            kps.append([0.0, 0.0, 0.0])\n",
    "                        else:\n",
    "                            point = list(point)\n",
    "                            point[-1] = 1.0\n",
    "                            kps.append(point)\n",
    "                    if len(kps) != 5:\n",
    "                        msg = f\"Image {image_path} has more or less than 5 kps: {kps}\"\n",
    "                        raise ValueError(msg)\n",
    "                    labels[\"key_points\"].append(kps)\n",
    "                    \n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> AnnotationDict:\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        key_points = label[\"key_points\"]\n",
    "        boxes = label[\"boxes\"]\n",
    "        return {\"image\": image, \"boxes\": boxes, \"key_points\": key_points}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for image_path, label in zip(self.images, self.labels):\n",
    "            yield {\"image\": image_path, **label}\n",
    "            \n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"data\", \"widerface\")\n",
    "dataset_df = None\n",
    "for subset in (\"train\", \"val\"):\n",
    "    anno_file_path = os.path.join(DATA_PATH, \"labelv2\", subset, \"labelv2.txt\")\n",
    "    image_dir = os.path.join(DATA_PATH, f\"WIDER_{subset}\", f\"WIDER_{subset}\", \"images\")\n",
    "    container = AnnotationContainer(image_dir, anno_file_path, subset == \"train\")\n",
    "    dataframe = pd.DataFrame(data=container)\n",
    "    dataframe[\"subset\"] = subset\n",
    "    if dataset_df is None:\n",
    "        dataset_df = dataframe\n",
    "    else:\n",
    "        dataset_df = pd.concat((dataset_df, dataframe))\n",
    "\n",
    "assert dataset_df is not None\n",
    "train_df = dataset_df.query(\"subset == 'train'\")\n",
    "train_idx = train_df.sample(frac=0.8).index\n",
    "val_idx = np.setdiff1d(train_df.index, train_idx)\n",
    "train_df.loc[train_idx, \"subset\"] = \"train\"\n",
    "train_df.loc[val_idx, \"subset\"] = \"val\"\n",
    "val_df = dataset_df.query(\"subset == 'val'\")\n",
    "val_df[\"subset\"] = \"test\"\n",
    "dataset_df = pd.concat((train_df, val_df)).reset_index(drop=True)\n",
    "dataset_df.to_csv(\"widerface_main_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdff39",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 245\u001b[39m\n\u001b[32m    243\u001b[39m running_losses: defaultdict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] = defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    244\u001b[39m yunet.train()\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1170\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1163\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\context.py:336\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     93\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     96\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from source.config import Config\n",
    "from train import read_config\n",
    "from source.postprocessing import postprocess_predictions\n",
    "from source.targets import generate_targets_batch\n",
    "from source.dataset import build_dataloaders\n",
    "from source.losses import DetectionLoss\n",
    "from source.models.yunet import YuNet\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from source.postprocessing import postprocess_predictions\n",
    "\n",
    "\n",
    "def lr_lambda(current_iter):\n",
    "    if current_iter >= warmup_iters:\n",
    "        return 1.0\n",
    "    # linear warmup от warmup_ratio до 1.0\n",
    "    alpha = current_iter / float(warmup_iters)\n",
    "    return warmup_ratio * (1 - alpha) + alpha\n",
    "\n",
    "\n",
    "def nan_hook(name):\n",
    "    def hook(module, inp, out):\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            outs = out\n",
    "        else:\n",
    "            outs = (out,)\n",
    "        for o in outs:\n",
    "            if torch.isnan(o).any() or torch.isinf(o).any():\n",
    "                print(f\"NaN in module {name}\")\n",
    "                raise RuntimeError(f\"NaN detected after {name}\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "config = read_config(\"config.yml\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "dataframe = pd.read_csv(config.path.csv)\n",
    "dataloaders = build_dataloaders(config, dataframe)\n",
    "model = YuNet(**config.model_dump()).to(device)\n",
    "handles = []\n",
    "for name, module in model.named_modules():\n",
    "    if len(list(module.children())) == 0:  # только \"листья\"\n",
    "        handles.append(module.register_forward_hook(nan_hook(name)))\n",
    "\n",
    "num_epochs = 80 * 8  # 640\n",
    "milestones = [50 * 8, 68 * 8]  # [400, 544]\n",
    "warmup_iters = 1500\n",
    "warmup_ratio = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "base_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1,)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "criterion = DetectionLoss(obj_weight=1.0, cls_weight=1.0, box_weight=5.0, kps_weight=0.1)\n",
    "\n",
    "global_iter = 0\n",
    "train_dataloader = dataloaders[\"train\"]\n",
    "for epoch in range(num_epochs):\n",
    "    running_losses: defaultdict[str, float] = defaultdict(float)\n",
    "    yunet.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        boxes = [item.to(device, non_blocking=True) for item in batch[\"boxes\"]]\n",
    "        kps = [item.to(device, non_blocking=True) for item in batch[\"key_points\"]]\n",
    "        p8_out, p16_out, p32_out = yunet(images)\n",
    "        obj_preds, cls_preds, box_preds, kps_preds, grids = postprocess_predictions((p8_out, p16_out, p32_out), (8, 16, 32))\n",
    "        foreground_mask, target_cls, target_obj, target_boxes, target_kps = generate_targets_batch(obj_preds, cls_preds, box_preds, grids, boxes, kps, device)\n",
    "        break\n",
    "    break\n",
    "    #     loss_dict: dict[str, torch.Tensor] = criterion(\n",
    "    #         (obj_preds, cls_preds, box_preds, kps_preds),\n",
    "    #         (target_obj, target_cls, target_boxes, target_kps),\n",
    "    #         foreground_mask,\n",
    "    #         grids,\n",
    "    #     )\n",
    "    #     loss = loss_dict[\"total_loss\"]\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     if global_iter < warmup_iters:\n",
    "    #         warmup_scheduler.step()\n",
    "    #     global_iter += 1\n",
    "\n",
    "    #     for loss_name, loss_tensor in loss_dict.items():\n",
    "    #         loss_value = loss_tensor.detach().cpu().item()\n",
    "    #         running_losses[f\"train_{loss_name}\"] += loss_value / len(train_dataloader)\n",
    "\n",
    "    # val_results = validate(yunet, dataloaders[\"val\"], device, score_thr=0.02, iou_thr=0.45)\n",
    "\n",
    "    # base_scheduler.step()\n",
    "    # loss_str = \", \".join([f\"{loss_name}={loss_value:.4f}\" for loss_name, loss_value in running_losses.items()])\n",
    "    # loss_str += \"|\" + \", \".join([f\"{name}={val:.4f}\" for name, val in val_results.items()])\n",
    "    # print(f\"[EPOCH {epoch + 1}/{num_epochs}] {loss_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
