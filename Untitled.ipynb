{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce80c86-76c4-4344-9a01-758fe50aee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\easyt\\AppData\\Local\\Temp\\ipykernel_206264\\2701843046.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df[\"subset\"] = \"test\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing as ty\n",
    "from itertools import islice\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batched_custom(iterable, n):\n",
    "    \"\"\"\n",
    "    Batches an iterable into chunks of size n.\n",
    "    Equivalent to itertools.batched in Python 3.12+.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class AnnotationDict(ty.TypedDict):\n",
    "    image: str\n",
    "    boxes: list[int]\n",
    "    key_points: list[list[int]]\n",
    "\n",
    "\n",
    "class AnnotationContainer:\n",
    "    def __init__(self, base_dir: str, anno_file_path: str, parse_kps: bool = False) -> None:\n",
    "        self.parse_kps = parse_kps\n",
    "        self.base_dir = base_dir\n",
    "        with open(anno_file_path, \"r\") as tf:\n",
    "            self.content = tf.read()\n",
    "        self.__parse()\n",
    "\n",
    "    def __parse(self) -> None:\n",
    "        self.meta = []\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        samples_as_text = self.content.split(\"# \")[1:] # We skip 1 element since it's an empty string.\n",
    "        samples_as_text = [sample.strip().split(\"\\n\") for sample in samples_as_text]\n",
    "        for sample in samples_as_text:\n",
    "            image_meta, *boxes_str = sample\n",
    "            \n",
    "            name, height, width = image_meta.split(\" \")\n",
    "            image_path = os.path.join(*name.split(\"/\")) # This should be correct for Windows and Unix.\n",
    "            self.images.append(image_path)\n",
    "            self.meta.append((height, width))\n",
    "\n",
    "            labels = {\"boxes\": [], \"key_points\": []}\n",
    "            for i, point_set in enumerate(boxes_str):\n",
    "                coords = list(map(float, point_set.strip().split(\" \")))\n",
    "\n",
    "                # Box is a first 4 coordinates in top_left_x, top_left_y, bottom_right_x, bottom_right_y format.\n",
    "                box = list(map(int, coords[:4]))\n",
    "\n",
    "                # Artificallly add class label to box.\n",
    "                box = [0, *box]\n",
    "                if any(coord < 0 for coord in box):\n",
    "                    msg = f\"Image {image_path} has box with negative coords: {box}\"\n",
    "                    raise ValueError(msg)\n",
    "                labels[\"boxes\"].append(box)\n",
    "\n",
    "                # Key points are the rest points. It should be 5 in total, 3 components each (x, y, visibility flag).\n",
    "                if self.parse_kps:\n",
    "                    kps = []\n",
    "                    for point in batched_custom(coords[4:], 3):\n",
    "                        if all(coord == -1 for coord in point):\n",
    "                            kps.append([0.0, 0.0, 0.0])\n",
    "                        else:\n",
    "                            point = list(point)\n",
    "                            point[-1] = 1.0\n",
    "                            kps.append(point)\n",
    "                    if len(kps) != 5:\n",
    "                        msg = f\"Image {image_path} has more or less than 5 kps: {kps}\"\n",
    "                        raise ValueError(msg)\n",
    "                    labels[\"key_points\"].append(kps)\n",
    "                    \n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> AnnotationDict:\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        key_points = label[\"key_points\"]\n",
    "        boxes = label[\"boxes\"]\n",
    "        return {\"image\": image, \"boxes\": boxes, \"key_points\": key_points}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for image_path, label in zip(self.images, self.labels):\n",
    "            yield {\"image\": image_path, **label}\n",
    "            \n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"data\", \"widerface\")\n",
    "dataset_df = None\n",
    "for subset in (\"train\", \"val\"):\n",
    "    anno_file_path = os.path.join(DATA_PATH, \"labelv2\", subset, \"labelv2.txt\")\n",
    "    image_dir = os.path.join(DATA_PATH, f\"WIDER_{subset}\", f\"WIDER_{subset}\", \"images\")\n",
    "    container = AnnotationContainer(image_dir, anno_file_path, subset == \"train\")\n",
    "    dataframe = pd.DataFrame(data=container)\n",
    "    dataframe[\"subset\"] = subset\n",
    "    if dataset_df is None:\n",
    "        dataset_df = dataframe\n",
    "    else:\n",
    "        dataset_df = pd.concat((dataset_df, dataframe))\n",
    "\n",
    "assert dataset_df is not None\n",
    "train_df = dataset_df.query(\"subset == 'train'\")\n",
    "train_idx = train_df.sample(frac=0.8).index\n",
    "val_idx = np.setdiff1d(train_df.index, train_idx)\n",
    "train_df.loc[train_idx, \"subset\"] = \"train\"\n",
    "train_df.loc[val_idx, \"subset\"] = \"val\"\n",
    "val_df = dataset_df.query(\"subset == 'val'\")\n",
    "val_df[\"subset\"] = \"test\"\n",
    "dataset_df = pd.concat((train_df, val_df)).reset_index(drop=True)\n",
    "dataset_df.to_csv(\"widerface_main_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcdff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\yunet_pytorch\\.venv\\Lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4319.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1/640] train_total_loss=3.6828, train_obj_loss=0.2111, train_cls_loss=0.7148, train_box_loss=2.6513, train_kps_loss=0.1056\n",
      "[EPOCH 2/640] train_total_loss=2.8116, train_obj_loss=0.0183, train_cls_loss=0.6760, train_box_loss=2.0322, train_kps_loss=0.0851\n",
      "[EPOCH 3/640] train_total_loss=2.5024, train_obj_loss=0.0167, train_cls_loss=0.6494, train_box_loss=1.7622, train_kps_loss=0.0741\n",
      "[EPOCH 4/640] train_total_loss=2.3020, train_obj_loss=0.0159, train_cls_loss=0.6276, train_box_loss=1.5919, train_kps_loss=0.0666\n",
      "[EPOCH 5/640] train_total_loss=2.2036, train_obj_loss=0.0151, train_cls_loss=0.6159, train_box_loss=1.5110, train_kps_loss=0.0616\n",
      "[EPOCH 6/640] train_total_loss=2.1630, train_obj_loss=0.0143, train_cls_loss=0.6109, train_box_loss=1.4801, train_kps_loss=0.0577\n",
      "[EPOCH 7/640] train_total_loss=2.1457, train_obj_loss=0.0139, train_cls_loss=0.6110, train_box_loss=1.4647, train_kps_loss=0.0561\n",
      "[EPOCH 8/640] train_total_loss=2.0946, train_obj_loss=0.0136, train_cls_loss=0.6034, train_box_loss=1.4253, train_kps_loss=0.0523\n",
      "[EPOCH 9/640] train_total_loss=2.0714, train_obj_loss=0.0133, train_cls_loss=0.6006, train_box_loss=1.4072, train_kps_loss=0.0503\n",
      "[EPOCH 10/640] train_total_loss=2.0513, train_obj_loss=0.0128, train_cls_loss=0.5974, train_box_loss=1.3915, train_kps_loss=0.0495\n",
      "[EPOCH 11/640] train_total_loss=2.0264, train_obj_loss=0.0127, train_cls_loss=0.5950, train_box_loss=1.3706, train_kps_loss=0.0480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m p8_out, p16_out, p32_out = model(images)\n\u001b[32m     75\u001b[39m obj_preds, cls_preds, box_preds, kps_preds, grids = postprocess_predictions((p8_out, p16_out, p32_out), (\u001b[32m8\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m32\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m foreground_mask, target_cls, target_obj, target_boxes, target_kps, kps_weights = \u001b[43mgenerate_targets_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m targets = (target_obj, target_cls, target_boxes, target_kps, kps_weights)\n\u001b[32m     78\u001b[39m inputs = (obj_preds, cls_preds, box_preds, kps_preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\source\\targets.py:125\u001b[39m, in \u001b[36mgenerate_targets_batch\u001b[39m\u001b[34m(obj_preds, cls_preds, box_preds, grids, gt_boxes, gt_kps, device)\u001b[39m\n\u001b[32m    121\u001b[39m per_image_targets = []\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# Targets as tuple:\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# (foreground_mask_i, target_cls_i, target_obj_i, target_boxes_i, target_kps_i, kps_weights_i)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     targets_i = \u001b[43mgenerate_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbox_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgt_kps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     per_image_targets.append([t.unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets_i])\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Transposing: now we have list of 6 lists of particular targets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\source\\targets.py:38\u001b[39m, in \u001b[36mgenerate_targets\u001b[39m\u001b[34m(cls_logits, obj_logits, boxes_xyxy, priors, gt_boxes, gt_labels, gt_kps)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m foreground_mask, target_cls, target_obj, target_boxes, target_kps, kps_weights\n\u001b[32m     34\u001b[39m offset_priors = torch.cat(\n\u001b[32m     35\u001b[39m     [priors[:, :\u001b[32m2\u001b[39m] + priors[:, \u001b[32m2\u001b[39m:] * \u001b[32m0.5\u001b[39m, priors[:, \u001b[32m2\u001b[39m:]], dim=-\u001b[32m1\u001b[39m\n\u001b[32m     36\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m assigned_gt_ids, assigned_labels, pos_ious = \u001b[43msimota_assign_per_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcls_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset_priors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mboxes_xyxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m foreground_mask = assigned_gt_ids >= \u001b[32m0\u001b[39m\n\u001b[32m     47\u001b[39m dtype = boxes_xyxy.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\source\\simota.py:209\u001b[39m, in \u001b[36msimota_assign_per_image\u001b[39m\u001b[34m(cls_probas, priors, boxes_xyxy, gt_boxes, gt_labels, r, topk, iou_weight, cls_weight)\u001b[39m\n\u001b[32m    206\u001b[39m cls_cost = cls_cost.sum(-\u001b[32m1\u001b[39m).T\n\u001b[32m    208\u001b[39m cost_matrix = cls_cost * cls_weight + iou_cost * iou_weight + (~in_gt_and_in_center_boxes) * \u001b[32m10_000\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m fg_mask_valid, matched_gt_valid, matched_ious_valid = \u001b[43mdynamic_k_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mious\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m valid_idx = valid_mask.nonzero(as_tuple=\u001b[38;5;28;01mFalse\u001b[39;00m).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    212\u001b[39m fg_valid_j = fg_mask_valid.nonzero(as_tuple=\u001b[38;5;28;01mFalse\u001b[39;00m).squeeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\yunet_pytorch\\source\\simota.py:100\u001b[39m, in \u001b[36mdynamic_k_matching\u001b[39m\u001b[34m(cost_matrix, ious, topk)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# It's okay if we have 1 GT for 1 valid box or 0 gt for 1 valid box.\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m matched_gt.sum() <= \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Match best GT in terms of cost with j-th valid predicted box.\u001b[39;00m\n\u001b[32m    103\u001b[39m gt_ids = torch.nonzero(matched_gt, as_tuple=\u001b[38;5;28;01mFalse\u001b[39;00m).squeeze(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from source.config import Config\n",
    "from train import read_config\n",
    "from source.postprocessing import postprocess_predictions\n",
    "from source.targets import generate_targets_batch\n",
    "from source.dataset import build_dataloaders\n",
    "from source.losses import DetectionLoss\n",
    "from source.models.yunet import YuNet\n",
    "from source.drawing import visualize_epoch_predictions\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from source.postprocessing import postprocess_predictions\n",
    "\n",
    "\n",
    "def lr_lambda(current_iter):\n",
    "    if current_iter >= warmup_iters:\n",
    "        return 1.0\n",
    "    # linear warmup от warmup_ratio до 1.0\n",
    "    alpha = current_iter / float(warmup_iters)\n",
    "    return warmup_ratio * (1 - alpha) + alpha\n",
    "\n",
    "\n",
    "def nan_hook(name):\n",
    "    def hook(module, inp, out):\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            outs = out\n",
    "        else:\n",
    "            outs = (out,)\n",
    "        for o in outs:\n",
    "            if torch.isnan(o).any() or torch.isinf(o).any():\n",
    "                print(f\"NaN in module {name}\")\n",
    "                raise RuntimeError(f\"NaN detected after {name}\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "config = read_config(\"config.yml\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "dataframe = pd.read_csv(config.path.csv)\n",
    "dataloaders = build_dataloaders(config, dataframe)\n",
    "model = YuNet(**config.model.model_dump()).to(device)\n",
    "handles = []\n",
    "for name, module in model.named_modules():\n",
    "    if len(list(module.children())) == 0:  # только \"листья\"\n",
    "        handles.append(module.register_forward_hook(nan_hook(name)))\n",
    "\n",
    "num_epochs = 80 * 8  # 640\n",
    "milestones = [50 * 8, 68 * 8]  # [400, 544]\n",
    "warmup_iters = 1500\n",
    "warmup_ratio = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "base_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1,)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "criterion = DetectionLoss(obj_weight=1.0, cls_weight=1.0, box_weight=5.0, kps_weight=0.1)\n",
    "\n",
    "global_iter = 0\n",
    "train_dataloader = dataloaders[\"train\"]\n",
    "for epoch in range(num_epochs):\n",
    "    running_losses: defaultdict[str, float] = defaultdict(float)\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        boxes = [item.to(device, non_blocking=True) for item in batch[\"boxes\"]]\n",
    "        kps = [item.to(device, non_blocking=True) for item in batch[\"key_points\"]]\n",
    "        p8_out, p16_out, p32_out = model(images)\n",
    "        obj_preds, cls_preds, box_preds, kps_preds, grids = postprocess_predictions((p8_out, p16_out, p32_out), (8, 16, 32))\n",
    "        foreground_mask, target_cls, target_obj, target_boxes, target_kps, kps_weights = generate_targets_batch(obj_preds, cls_preds, box_preds, grids, boxes, kps, device)\n",
    "        targets = (target_obj, target_cls, target_boxes, target_kps, kps_weights)\n",
    "        inputs = (obj_preds, cls_preds, box_preds, kps_preds)\n",
    "        loss_dict: dict[str, torch.Tensor] = criterion(inputs, targets, foreground_mask, grids)\n",
    "        loss = loss_dict[\"total_loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if global_iter < warmup_iters:\n",
    "            warmup_scheduler.step()\n",
    "        global_iter += 1\n",
    "\n",
    "        for loss_name, loss_tensor in loss_dict.items():\n",
    "            loss_value = loss_tensor.detach().cpu().item()\n",
    "            running_losses[f\"train_{loss_name}\"] += loss_value / len(train_dataloader)\n",
    "\n",
    "    base_scheduler.step()\n",
    "    visualize_epoch_predictions(\n",
    "        \"artifacts\",\n",
    "        epoch,\n",
    "        model,\n",
    "        next(iter(dataloaders[\"val\"])),\n",
    "        device,\n",
    "    )\n",
    "    loss_str = \", \".join([f\"{loss_name}={loss_value:.4f}\" for loss_name, loss_value in running_losses.items()])\n",
    "    print(f\"[EPOCH {epoch + 1}/{num_epochs}] {loss_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
